{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8386cb2d-7b5e-4082-9d1c-2ad659b5c2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "MLP(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=2, w=2, depth=2, shp=None):\n",
    "        super(MLP, self).__init__()\n",
    "        if shp == None:\n",
    "            shp = [in_dim] + [w]*(depth-1) + [out_dim]\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.depth = depth\n",
    "                 \n",
    "        else:\n",
    "            self.in_dim = shp[0]\n",
    "            self.out_dim = shp[-1]\n",
    "            self.depth = len(shp) - 1\n",
    "        linear_list = []\n",
    "        for i in range(self.depth):\n",
    "            linear_list.append(nn.Linear(shp[i], shp[i+1]))\n",
    "        self.linears = nn.ModuleList(linear_list)\n",
    "        self.shp = shp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input shape = (batch_size, input_dim)\n",
    "        # define activation here\n",
    "        #f = lambda x: 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "        f = torch.nn.SiLU()\n",
    "        for i in range(self.depth-1):\n",
    "            x = f(self.linears[i](x))\n",
    "        x = self.linears[-1](x)\n",
    "        # output shape = (batch_size, output_dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "### initialize an MLP ###\n",
    "# Method 1: via shape\n",
    "shp = [2, 50, 1] # 2 input dim, 1 hidden layer with width 50, 1 output dim\n",
    "model = MLP(shp=shp)\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Method 2: via in_dim, out_dim, width, depth (assume all hidden layers to have the same width)\n",
    "# Note: depth here = the number of linear matrices = one plus the number of hidden layers\n",
    "in_dim = 2 # 2 input dim\n",
    "out_dim = 1 # 1 output dim\n",
    "w = 50 # width = 50\n",
    "depth = 2 # 2 linear matrices = 1 hidden layer\n",
    "model = MLP(in_dim=in_dim, out_dim=out_dim, w=w, depth=depth)\n",
    "print(model)\n",
    "\n",
    "\n",
    "### feed data to MLP ###\n",
    "batch_size = 128\n",
    "input_dim = 2\n",
    "x = torch.normal(0,1,size=(batch_size, input_dim))\n",
    "model(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
