{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35fba670-3b49-4eaa-bfcd-876e1ea222b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16, 19])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# this implementation follows minGPT: https://github.com/karpathy/minGPT\n",
    "\n",
    "# this transformer take in token ids, e.g., \"She has a cat.\" => [\"She\", \"has\", \"a\", \"cat\"] => [0, 1, 2, 3]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=2, w=2, depth=2, shp=None):\n",
    "        super(MLP, self).__init__()\n",
    "        if shp == None:\n",
    "            shp = [in_dim] + [w]*(depth-1) + [out_dim]\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.depth = depth\n",
    "                 \n",
    "        else:\n",
    "            self.in_dim = shp[0]\n",
    "            self.out_dim = shp[-1]\n",
    "            self.depth = len(shp) - 1\n",
    "        linear_list = []\n",
    "        for i in range(self.depth):\n",
    "            linear_list.append(nn.Linear(shp[i], shp[i+1]))\n",
    "        self.linears = nn.ModuleList(linear_list)\n",
    "        self.shp = shp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = lambda x: 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "        #f = torch.nn.SiLU()\n",
    "        for i in range(self.depth-1):\n",
    "            x = f(self.linears[i](x))\n",
    "        x = self.linears[-1](x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, n_head=2, n_embed=6):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.l_attn = nn.Linear(n_embed, 3*n_embed)\n",
    "        # output projection\n",
    "        self.l_proj = nn.Linear(n_embed, n_embed)\n",
    "        self.n_head = n_head\n",
    "        self.n_embed = n_embed\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B: batch size; T: sequence length; C: embedding dimensionality (n_embd)\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # query, key, value\n",
    "        x = self.l_attn(x)\n",
    "        q, k, v = x[:,:,:C], x[:,:,C:2*C], x[:,:,2*C:3*C]\n",
    "        n_head = self.n_head\n",
    "        assert C % n_head == 0\n",
    "        q = q.reshape(B, T, n_head, int(C/n_head))\n",
    "        k = k.reshape(B, T, n_head, int(C/n_head))\n",
    "        v = v.reshape(B, T, n_head, int(C/n_head))\n",
    "\n",
    "        # (causal) self-attention\n",
    "        attn = torch.einsum('ijhl,ikhl->ijkh', q, k)/np.sqrt(int(C/n_head))\n",
    "        mask = torch.ones(T,T)*float('-inf')\n",
    "        mask = torch.tril(mask, diagonal=-1).permute(1,0).unsqueeze(dim=0).unsqueeze(dim=3)\n",
    "        attn = attn + mask\n",
    "        attn = nn.Softmax(dim=2)(attn)\n",
    "        attn = torch.einsum('ijkl,iklh->ijlh', attn, v)\n",
    "        attn = attn.reshape(B, T, C)\n",
    "\n",
    "        # output projection\n",
    "        y = self.l_proj(attn)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    # A transformer block\n",
    "    def __init__(self, n_head=2, n_embed=6):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_embed = n_embed\n",
    "        self.ln_1 = nn.LayerNorm(n_embed)\n",
    "        self.attn = Attention(n_head=n_head, n_embed=n_embed)\n",
    "        self.ln_2 = nn.LayerNorm(n_embed)\n",
    "        self.mlp = MLP(shp=[n_embed, 4*n_embed, n_embed])\n",
    "\n",
    "    def forward(self, x):\n",
    "        #If you want to use layer norm, use this\n",
    "        #x = x + self.attn(self.ln_1(x))\n",
    "        #x = x + self.mlp(self.ln_2(x))\n",
    "        #x = self.attn(self.ln_1(x))\n",
    "        #x = self.mlp(self.ln_2(x))\n",
    "        \n",
    "        #If you don't want to use layer norm, use this\n",
    "        x = x + self.attn(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    # Transformer: since our goal is to deal with linear regression, not language, \n",
    "    # we ignore token embeddings and positioanl embeddings. \n",
    "    def __init__(self, out_dim=19, n_head=2, n_embed=20, n_layer=2):\n",
    "        super().__init__()\n",
    "        self.n_head = n_head\n",
    "        self.n_embed = n_embed\n",
    "        self.n_layer = n_layer\n",
    "        self.l_i = nn.Linear(n_embed, n_embed)\n",
    "        self.blocks = nn.ModuleList([Block(n_head=n_head, n_embed=n_embed) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)\n",
    "        self.l_f = nn.Linear(n_embed, out_dim)\n",
    "        self.in_dim = n_embed\n",
    "        self.out_dim = out_dim\n",
    "        self.embedding = nn.Parameter(torch.normal(0,1,size=(out_dim,n_embed)))\n",
    "        \n",
    "    def forward(self, token_ids):\n",
    "        # token_ids shape: (batch_size, sequence length)\n",
    "        x = self.embedding[token_ids]\n",
    "        x = self.l_i(x)\n",
    "        for i in range(self.n_layer):\n",
    "            x = self.blocks[i](x)\n",
    "        y = self.l_f(x)\n",
    "        # y shape: (batch_size, sequence length, out_dim)\n",
    "        # here out_dim is the number of tokens in your library\n",
    "        # y is the logits used to predict the next token\n",
    "        return y\n",
    "\n",
    "# initialize a transformer\n",
    "out_dim = 19\n",
    "n_head = 4\n",
    "n_embed = 32\n",
    "n_layer = 2\n",
    "model = Transformer(out_dim=out_dim, n_head=n_head, n_embed=n_embed, n_layer=n_layer)\n",
    "\n",
    "# feed data into transformer\n",
    "batch_size = 128\n",
    "seq_len = 16\n",
    "x = np.random.choice(out_dim,size=(batch_size, seq_len))\n",
    "model(x).shape # (batch_size, seq_len, out_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
