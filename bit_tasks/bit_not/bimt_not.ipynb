{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9639e060-f0d3-43f3-b016-c3675521c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os \n",
    "\n",
    "torch.set_default_dtype(torch.double)\n",
    "\n",
    "\n",
    "##### Define BIMT networks #####\n",
    "class BioLinear(nn.Module):\n",
    "    # BioLinear is just Linear, but each neuron comes with coordinates.\n",
    "    def __init__(self, in_dim, out_dim, in_fold=1, out_fold=1):\n",
    "        super(BioLinear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.in_fold = in_fold # in_fold is the number of folds applied to input vectors. It only affects coordinates, not computations.\n",
    "        self.out_fold = out_fold # out_fold is the number of folds applied to output vectors. It only affects coordinates, not computations.\n",
    "        assert in_dim % in_fold == 0\n",
    "        assert out_dim % out_fold == 0\n",
    "        #compute in_cor, shape: (in_dim)\n",
    "        in_dim_fold = int(in_dim/in_fold)\n",
    "        out_dim_fold = int(out_dim/out_fold)\n",
    "        self.in_coordinates = torch.tensor(list(np.linspace(1/(2*in_dim_fold), 1-1/(2*in_dim_fold), num=in_dim_fold))*in_fold, dtype=torch.float) # place input neurons in 1D Euclidean space\n",
    "        self.out_coordinates = torch.tensor(list(np.linspace(1/(2*out_dim_fold), 1-1/(2*out_dim_fold), num=out_dim_fold))*out_fold, dtype=torch.float) # place output neurons in 1D Euclidean space\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x.clone()\n",
    "        self.output = self.linear(x).clone()\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "class BioMLP(nn.Module):\n",
    "    # BioMLP is just MLP, but each neuron comes with coordinates.\n",
    "    def __init__(self, in_dim=2, out_dim=2, w=2, depth=2, shp=None, token_embedding=False, embedding_size=None):\n",
    "        super(BioMLP, self).__init__()\n",
    "        if shp == None:\n",
    "            shp = [in_dim] + [w]*(depth-1) + [out_dim]\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.depth = depth\n",
    "                 \n",
    "        else:\n",
    "            self.in_dim = shp[0]\n",
    "            self.out_dim = shp[-1]\n",
    "            self.depth = len(shp) - 1\n",
    "\n",
    "        linear_list = []\n",
    "        for i in range(self.depth):\n",
    "            if i == 0:\n",
    "                linear_list.append(BioLinear(shp[i], shp[i+1], in_fold=1))\n",
    "                \n",
    "            else:\n",
    "                linear_list.append(BioLinear(shp[i], shp[i+1]))\n",
    "        self.linears = nn.ModuleList(linear_list)\n",
    "        \n",
    "        \n",
    "        if token_embedding == True:\n",
    "            # embedding size: number of tokens * embedding dimension\n",
    "            self.embedding = torch.nn.Parameter(torch.normal(0,1,size=embedding_size))\n",
    "        \n",
    "        self.shp = shp\n",
    "        # parameters for the bio-inspired trick\n",
    "        self.l0 = 0.1 # distance between two nearby layers\n",
    "        self.in_perm = torch.nn.Parameter(torch.tensor(np.arange(int(self.in_dim/self.linears[0].in_fold)), dtype=torch.float))\n",
    "        self.out_perm = torch.nn.Parameter(torch.tensor(np.arange(int(self.out_dim/self.linears[-1].out_fold)), dtype=torch.float))\n",
    "        self.top_k = 5 # the number of important neurons (used in Swaps)\n",
    "        self.token_embedding = token_embedding\n",
    "        self.n_parameters = sum(p.numel() for p in self.parameters())\n",
    "        self.original_params = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shp = x.shape\n",
    "        in_fold = self.linears[0].in_fold\n",
    "        x = x.reshape(shp[0], in_fold, int(shp[1]/in_fold))\n",
    "        x = x[:,:,self.in_perm.long()]\n",
    "        x = x.reshape(shp[0], shp[1])\n",
    "        #f = torch.nn.SiLU()\n",
    "        f = torch.nn.ReLU()\n",
    "        for i in range(self.depth-1):\n",
    "            x = f(self.linears[i](x))\n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        out_perm_inv = torch.zeros(self.out_dim, dtype=torch.long)\n",
    "        out_perm_inv[self.out_perm.long()] = torch.arange(self.out_dim)\n",
    "        x = x[:,out_perm_inv]\n",
    "        #x = x[:,self.out_perm]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_linear_layers(self):\n",
    "        return self.linears\n",
    "    \n",
    "    def get_cc(self, weight_factor=1.0, bias_penalize=True, no_penalize_last=False):\n",
    "        # compute connection cost\n",
    "        # bias_penalize = True penalizes biases, otherwise doesn't penalize biases\n",
    "        # no_penalize_last = True means do not penalize last linear layer, False means penalize last layer.\n",
    "        cc = 0\n",
    "        num_linear = len(self.linears)\n",
    "        for i in range(num_linear):\n",
    "            if i == num_linear - 1 and no_penalize_last:\n",
    "                weight_factor = 0.\n",
    "            biolinear = self.linears[i]\n",
    "            dist = torch.abs(biolinear.out_coordinates.unsqueeze(dim=1) - biolinear.in_coordinates.unsqueeze(dim=0))\n",
    "            cc += torch.sum(torch.abs(biolinear.linear.weight)*(weight_factor*dist+self.l0))\n",
    "            if bias_penalize == True:\n",
    "                cc += torch.sum(torch.abs(biolinear.linear.bias)*(self.l0))\n",
    "        if self.token_embedding:\n",
    "            cc += torch.sum(torch.abs(self.embedding)*(self.l0))\n",
    "            #pass\n",
    "        return cc\n",
    "    \n",
    "    def swap_weight(self, weights, j, k, swap_type=\"out\"):\n",
    "        # Given a weight matrix, swap the j^th and k^th neuron in inputs/outputs when swap_type = \"in\"/\"out\"\n",
    "        with torch.no_grad():  \n",
    "            if swap_type == \"in\":\n",
    "                temp = weights[:,j].clone()\n",
    "                weights[:,j] = weights[:,k].clone()\n",
    "                weights[:,k] = temp\n",
    "            elif swap_type == \"out\":\n",
    "                temp = weights[j].clone()\n",
    "                weights[j] = weights[k].clone()\n",
    "                weights[k] = temp\n",
    "            else:\n",
    "                raise Exception(\"Swap type {} is not recognized!\".format(swap_type))\n",
    "            \n",
    "    def swap_bias(self, biases, j, k):\n",
    "        # Given a bias vector, swap the j^th and k^th neuron.\n",
    "        with torch.no_grad():  \n",
    "            temp = biases[j].clone()\n",
    "            biases[j] = biases[k].clone()\n",
    "            biases[k] = temp\n",
    "    \n",
    "    def swap(self, i, j, k):\n",
    "        # in the ith layer (of neurons), swap the jth and the kth neuron. \n",
    "        # Note: n layers of weights means n+1 layers of neurons.\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            # input layer, only has outgoing weights; update in_perm\n",
    "            weights = linears[i].linear.weight\n",
    "            infold = linears[i].in_fold\n",
    "            fold_dim = int(weights.shape[1]/infold)\n",
    "            for l in range(infold):\n",
    "                self.swap_weight(weights, j+fold_dim*l, k+fold_dim*l, swap_type=\"in\")\n",
    "            # change input_perm\n",
    "            self.swap_bias(self.in_perm, j, k)\n",
    "        elif i == num_linear:\n",
    "            # output layer, only has incoming weights and biases; update out_perm\n",
    "            weights = linears[i-1].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights, j, k, swap_type=\"out\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "            # change output_perm\n",
    "            self.swap_bias(self.out_perm, j, k)\n",
    "        else:\n",
    "            # middle layer : incoming weights, outgoing weights, and biases\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights_in, j, k, swap_type=\"out\")\n",
    "            self.swap_weight(weights_out, j, k, swap_type=\"in\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "\n",
    "    def get_top_id(self, i, top_k=20):\n",
    "        # in the ith layer (of neurons), get the top k important neurons (have large weight connections with other neurons)\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            # input layer\n",
    "            weights = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=0)\n",
    "            in_fold = linears[0].in_fold\n",
    "            #print(score.shape)\n",
    "            score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "        elif i == num_linear:\n",
    "            # output layer\n",
    "            weights = linears[i-1].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=1)\n",
    "        else:\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "        #print(score.shape)\n",
    "        top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "        return top_index\n",
    "    \n",
    "    def relocate_ij(self, i, j):\n",
    "        # In the ith layer (of neurons), relocate the jth neuron\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i < num_linear:\n",
    "            num_neuron = int(linears[i].linear.weight.shape[1]/linears[i].in_fold)\n",
    "        else:\n",
    "            num_neuron = linears[i-1].linear.weight.shape[0]\n",
    "        ccs = []\n",
    "        for k in range(num_neuron):\n",
    "            self.swap(i,j,k)\n",
    "            ccs.append(self.get_cc())\n",
    "            self.swap(i,j,k)\n",
    "        k = torch.argmin(torch.stack(ccs))\n",
    "        self.swap(i,j,k)\n",
    "            \n",
    "    def relocate_i(self, i):\n",
    "        # Relocate neurons in the ith layer\n",
    "        top_id = self.get_top_id(i, top_k=self.top_k)\n",
    "        for j in top_id:\n",
    "            self.relocate_ij(i,j)\n",
    "            \n",
    "    def relocate(self):\n",
    "        # Relocate neurons in the whole model\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        for i in range(num_linear+1):\n",
    "            self.relocate_i(i)\n",
    "            \n",
    "    def plot(self, mode=\"internal\", model=None):\n",
    "        fig, ax = plt.subplots(figsize=(3,3))\n",
    "        #ax = plt.gca()\n",
    "        shp = self.shp\n",
    "        s = 1/(2*max(shp))\n",
    "        for j in range(len(shp)):\n",
    "            N = shp[j]\n",
    "            if j == 0:\n",
    "                in_fold = self.linears[j].in_fold\n",
    "                N = int(N/in_fold)\n",
    "            for i in range(N):\n",
    "                if j == 0:\n",
    "                    for fold in range(in_fold):\n",
    "                        circle = Ellipse((1/(2*N)+i/N, 0.1*j+0.02*fold-0.01), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                        ax.add_patch(circle)\n",
    "                else:\n",
    "                    for fold in range(in_fold):\n",
    "                        circle = Ellipse((1/(2*N)+i/N, 0.1*j), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                        ax.add_patch(circle)\n",
    "\n",
    "\n",
    "        plt.ylim(-0.02,0.1*(len(shp)-1)+0.02)\n",
    "        plt.xlim(-0.02,1.02)\n",
    "\n",
    "        if mode == \"internal\":\n",
    "            linears = self.linears\n",
    "        else:\n",
    "            linears = model.linears\n",
    "        for ii in range(len(linears)):\n",
    "            biolinear = linears[ii]\n",
    "            p = biolinear.linear.weight\n",
    "            p_shp = p.shape\n",
    "            p = p/torch.abs(p).max()\n",
    "            in_fold = biolinear.in_fold\n",
    "            fold_num = int(p_shp[1]/in_fold)\n",
    "            for i in range(p_shp[0]):\n",
    "                if ii == 0:\n",
    "                    for fold in range(in_fold):\n",
    "                        for j in range(fold_num):\n",
    "                            plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*fold_num)+j/fold_num], [0.1*(ii+1),0.1*ii+0.02*fold-0.01], lw=np.minimum(1*np.abs(p[i,j].detach().numpy()), 1), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "                else:\n",
    "                    for j in range(fold_num):\n",
    "                        plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*fold_num)+j/fold_num], [0.1*(ii+1),0.1*ii], lw=np.minimum(1*np.abs(p[i,j].detach().numpy()), 1), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "                    \n",
    "        ax.axis('off')\n",
    "        \n",
    "        \n",
    "    def plot_weights(self, mode=\"internal\", model=None):\n",
    "        fig, ax = plt.subplots(figsize=(3,3))\n",
    "        #ax = plt.gca()\n",
    "        shp = self.shp\n",
    "        s = 1/(2*max(shp))\n",
    "        for j in range(len(shp)):\n",
    "            N = shp[j]\n",
    "            if j == 0:\n",
    "                in_fold = self.linears[j].in_fold\n",
    "                N = int(N/in_fold)\n",
    "            for i in range(N):\n",
    "                if j == 0:\n",
    "                    for fold in range(in_fold):\n",
    "                        circle = Ellipse((1/(2*N)+i/N, 0.1*j+0.02*fold-0.01), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                        ax.add_patch(circle)\n",
    "                else:\n",
    "                    for fold in range(in_fold):\n",
    "                        circle = Ellipse((1/(2*N)+i/N, 0.1*j), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                        ax.add_patch(circle)\n",
    "\n",
    "\n",
    "        plt.ylim(-0.02,0.1*(len(shp)-1)+0.02)\n",
    "        plt.xlim(-0.02,1.02)\n",
    "\n",
    "        if mode == \"internal\":\n",
    "            linears = self.linears\n",
    "        elif mode == \"external\":\n",
    "            linears = model.linears\n",
    "        for ii in range(len(linears)):\n",
    "            biolinear = linears[ii]\n",
    "            p = biolinear.linear.weight\n",
    "            p_shp = p.shape\n",
    "            p = p/torch.abs(p).max()\n",
    "            in_fold = biolinear.in_fold\n",
    "            fold_num = int(p_shp[1]/in_fold)\n",
    "            for i in range(p_shp[0]):\n",
    "                if ii == 0:\n",
    "                    for fold in range(in_fold):\n",
    "                        for j in range(fold_num):\n",
    "                            plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*fold_num)+j/fold_num], [0.1*(ii+1),0.1*ii+0.02*fold-0.01], lw=np.minimum(1*np.abs(p[i,j].detach().numpy()), 1), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "                else:\n",
    "                    for j in range(fold_num):\n",
    "                        plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*fold_num)+j/fold_num], [0.1*(ii+1),0.1*ii], lw=np.minimum(1*np.abs(p[i,j].detach().numpy()), 1), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "                    \n",
    "        ax.axis('off')\n",
    "        \n",
    "        \n",
    "    def thresholding(self, threshold, checkpoint = True):\n",
    "        # snap too small weights (smaller than threshold) to zero. Useful for pruning.\n",
    "        num = 0\n",
    "        if checkpoint:\n",
    "            self.original_params = [param.clone() for param in self.parameters()]\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                num += torch.sum(torch.abs(param)>threshold)\n",
    "                param.data = param*(torch.abs(param)>threshold)\n",
    "        return num\n",
    "                \n",
    "    def intervening(self, i, pos, value, ptype=\"weight\", checkpoint = True):\n",
    "        if checkpoint:\n",
    "            self.original_params = [param.clone() for param in self.parameters()]\n",
    "        with torch.no_grad():\n",
    "            if ptype == \"weight\":\n",
    "                self.linears[i].linear.weight[pos] = value\n",
    "            elif ptype == \"bias\":\n",
    "                self.linears[i].linear.bias[pos] = value\n",
    "                \n",
    "    def revert(self):\n",
    "        with torch.no_grad():\n",
    "            for param, original_param in zip(self.parameters(), self.original_params):\n",
    "                param.data.copy_(original_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96735974-5b09-454f-b70e-d49ae18ca4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### define datasets #####\n",
    "# # mult-task sparse parity\n",
    "\n",
    "# p = 2 #modulo p\n",
    "# n_control = 4 #number of control bits = number of tasks\n",
    "# n_task = 8 #number of task bits\n",
    "# k = 2 # sparse k; add k numbers\n",
    "# indices = np.array([[0,1],[2,3],[4,5],[6,7]]) # indicate indices for tasks bits for each task\n",
    "\n",
    "# assert indices.shape[0] == n_control\n",
    "# assert indices.shape[1] == k\n",
    "\n",
    "# def num2bin(x):\n",
    "#     arr = []\n",
    "#     for i in range(n_task):\n",
    "#         k = (x/p).astype(int)\n",
    "#         r = x % p\n",
    "#         x = k\n",
    "#         arr.append(r)\n",
    "#     return arr\n",
    "        \n",
    "# task_bits = np.transpose(np.array(num2bin(np.arange(p**n_task))))\n",
    "# target = np.sum(task_bits[:,indices], axis=2) % p\n",
    "\n",
    "# intput_bits = []\n",
    "# label_bits = []\n",
    "# for i in range(n_control):\n",
    "#     new_bits = np.concatenate([np.eye(n_control)[np.array([i] * task_bits.shape[0])], task_bits], axis=1)\n",
    "#     if i == 0:\n",
    "#         input_bits = new_bits\n",
    "#         label_bits = target[:,i]\n",
    "#     else:\n",
    "#         input_bits = np.concatenate([input_bits, new_bits], axis=0)\n",
    "#         label_bits = np.concatenate([label_bits, target[:,i]], axis=0)\n",
    "\n",
    "# # For other tasks, write your own create_dataset function to generate inputs and labels\n",
    "# inputs = torch.tensor(input_bits, dtype=torch.double, requires_grad=True)\n",
    "# labels = torch.tensor(label_bits, dtype=torch.double, requires_grad=True).unsqueeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bffc34-f160-494b-824a-3250f5e81d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"bit_not\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\" #HACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "839299ee-c77c-4410-adf4-50db27a9f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mode='train'):\n",
    "    data = np.loadtxt(f'../{version}/data_{mode}.txt', dtype='str')\n",
    "    inputs = data[:,:1]\n",
    "    labels = data[:,1]\n",
    "\n",
    "    def strs2mat(strings):\n",
    "        num = strings.shape[0]\n",
    "        mat = []\n",
    "        for i in range(num):\n",
    "            mat.append([*strings[i]])\n",
    "        return mat\n",
    "\n",
    "    inputs_ = np.array([strs2mat(inputs[:,0])]).astype('float').transpose(1,2,0)\n",
    "    labels_ = np.array(strs2mat(labels))[:,:,np.newaxis].astype('float')\n",
    "\n",
    "    return inputs_, labels_\n",
    "\n",
    "inputs_train, labels_train = load_data(mode='train')\n",
    "inputs_test, labels_test = load_data(mode='test')\n",
    "\n",
    "inputs_train = torch.tensor(inputs_train, dtype=torch.float64, requires_grad=True).to(device)\n",
    "labels_train = torch.tensor(labels_train, dtype=torch.float64, requires_grad=True).to(device)\n",
    "inputs_test = torch.tensor(inputs_test, dtype=torch.float64, requires_grad=True).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.float64, requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d95dd12-a2d8-435d-a227-8f0e7c7b026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = inputs_train.reshape(819,-1), labels_train.reshape(819,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38d13263-1083-414b-b6e0-dab1cddd5b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([819, 6]), torch.Size([819, 6]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "432052f5-af49-42cb-9055-0ca2cf4b39a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 | total loss: 5.77e-01 | train loss: 5.42e-01 | reg: 3.57e+01 \n",
      "step = 200 | total loss: 3.46e-02 | train loss: 1.36e-03 | reg: 3.33e+01 \n",
      "step = 400 | total loss: 2.35e-02 | train loss: 3.41e-04 | reg: 2.32e+01 \n",
      "step = 600 | total loss: 1.63e-02 | train loss: 1.31e-04 | reg: 1.62e+01 \n",
      "step = 800 | total loss: 1.21e-02 | train loss: 5.68e-05 | reg: 1.21e+01 \n",
      "step = 1000 | total loss: 8.94e-03 | train loss: 2.66e-05 | reg: 8.92e+00 \n",
      "step = 1200 | total loss: 6.99e-03 | train loss: 1.46e-05 | reg: 6.98e+00 \n",
      "step = 1400 | total loss: 5.77e-03 | train loss: 1.07e-05 | reg: 5.76e+00 \n",
      "step = 1600 | total loss: 5.04e-03 | train loss: 7.87e-06 | reg: 5.03e+00 \n",
      "step = 1800 | total loss: 4.48e-03 | train loss: 6.76e-06 | reg: 4.48e+00 \n",
      "step = 2000 | total loss: 4.11e-03 | train loss: 4.23e-06 | reg: 4.11e+00 \n",
      "step = 2200 | total loss: 3.85e-03 | train loss: 3.78e-06 | reg: 3.84e+00 \n",
      "step = 2400 | total loss: 3.59e-03 | train loss: 3.14e-06 | reg: 3.58e+00 \n",
      "step = 2600 | total loss: 3.33e-03 | train loss: 3.21e-06 | reg: 3.33e+00 \n",
      "step = 2800 | total loss: 3.01e-03 | train loss: 2.58e-06 | reg: 3.01e+00 \n",
      "step = 3000 | total loss: 2.83e-03 | train loss: 2.04e-06 | reg: 2.83e+00 \n",
      "step = 3200 | total loss: 2.72e-03 | train loss: 1.60e-06 | reg: 2.72e+00 \n",
      "step = 3400 | total loss: 2.62e-03 | train loss: 1.62e-06 | reg: 2.61e+00 \n",
      "step = 3600 | total loss: 2.50e-03 | train loss: 1.32e-06 | reg: 2.50e+00 \n",
      "step = 3800 | total loss: 2.43e-03 | train loss: 1.56e-06 | reg: 2.42e+00 \n",
      "step = 4000 | total loss: 2.38e-03 | train loss: 1.23e-06 | reg: 2.38e+00 \n",
      "step = 4200 | total loss: 2.35e-03 | train loss: 1.70e-06 | reg: 2.35e+00 \n",
      "step = 4400 | total loss: 2.33e-03 | train loss: 1.57e-06 | reg: 2.33e+00 \n",
      "step = 4600 | total loss: 2.33e-03 | train loss: 1.74e-06 | reg: 2.33e+00 \n",
      "step = 4800 | total loss: 2.32e-03 | train loss: 1.84e-06 | reg: 2.31e+00 \n",
      "step = 5000 | total loss: 2.31e-03 | train loss: 1.57e-06 | reg: 2.31e+00 \n",
      "step = 5200 | total loss: 2.31e-03 | train loss: 1.70e-06 | reg: 2.31e+00 \n",
      "step = 5400 | total loss: 2.31e-03 | train loss: 1.47e-06 | reg: 2.30e+00 \n",
      "step = 5600 | total loss: 2.30e-03 | train loss: 1.17e-06 | reg: 2.30e+00 \n",
      "step = 5800 | total loss: 2.30e-03 | train loss: 1.46e-06 | reg: 2.30e+00 \n",
      "step = 6000 | total loss: 2.30e-03 | train loss: 1.60e-06 | reg: 2.29e+00 \n",
      "step = 6200 | total loss: 2.29e-03 | train loss: 2.03e-06 | reg: 2.29e+00 \n",
      "step = 6400 | total loss: 2.29e-03 | train loss: 1.91e-06 | reg: 2.28e+00 \n",
      "step = 6600 | total loss: 2.28e-03 | train loss: 1.59e-06 | reg: 2.28e+00 \n",
      "step = 6800 | total loss: 2.27e-03 | train loss: 1.81e-06 | reg: 2.27e+00 \n",
      "step = 7000 | total loss: 2.26e-03 | train loss: 1.35e-06 | reg: 2.26e+00 \n",
      "step = 7200 | total loss: 2.25e-03 | train loss: 1.41e-06 | reg: 2.25e+00 \n",
      "step = 7400 | total loss: 2.24e-03 | train loss: 1.34e-06 | reg: 2.24e+00 \n",
      "step = 7600 | total loss: 2.23e-03 | train loss: 1.54e-06 | reg: 2.23e+00 \n",
      "step = 7800 | total loss: 2.21e-03 | train loss: 2.42e-06 | reg: 2.21e+00 \n",
      "step = 8000 | total loss: 2.19e-03 | train loss: 2.35e-06 | reg: 2.19e+00 \n",
      "step = 8200 | total loss: 2.18e-03 | train loss: 1.36e-06 | reg: 2.18e+00 \n",
      "step = 8400 | total loss: 2.16e-03 | train loss: 1.43e-06 | reg: 2.16e+00 \n",
      "step = 8600 | total loss: 2.15e-03 | train loss: 1.31e-06 | reg: 2.15e+00 \n",
      "step = 8800 | total loss: 2.13e-03 | train loss: 1.34e-06 | reg: 2.13e+00 \n",
      "step = 9000 | total loss: 2.11e-03 | train loss: 1.09e-06 | reg: 2.11e+00 \n",
      "step = 9200 | total loss: 2.05e-03 | train loss: 1.39e-06 | reg: 2.05e+00 \n",
      "step = 9400 | total loss: 2.00e-03 | train loss: 1.24e-06 | reg: 1.99e+00 \n",
      "step = 9600 | total loss: 1.99e-03 | train loss: 1.06e-06 | reg: 1.99e+00 \n",
      "step = 9800 | total loss: 1.99e-03 | train loss: 1.48e-06 | reg: 1.99e+00 \n",
      "step = 10000 | total loss: 2.00e-03 | train loss: 1.27e-06 | reg: 1.99e+00 \n",
      "step = 10200 | total loss: 1.99e-03 | train loss: 1.08e-06 | reg: 1.99e+00 \n",
      "step = 10400 | total loss: 1.99e-03 | train loss: 1.27e-06 | reg: 1.99e+00 \n",
      "step = 10600 | total loss: 1.99e-03 | train loss: 1.66e-06 | reg: 1.99e+00 \n",
      "step = 10800 | total loss: 1.99e-03 | train loss: 1.62e-06 | reg: 1.99e+00 \n",
      "step = 11000 | total loss: 1.98e-03 | train loss: 1.06e-06 | reg: 1.98e+00 \n",
      "step = 11200 | total loss: 1.98e-03 | train loss: 1.13e-06 | reg: 1.98e+00 \n",
      "step = 11400 | total loss: 1.97e-03 | train loss: 1.19e-06 | reg: 1.97e+00 \n",
      "step = 11600 | total loss: 1.96e-03 | train loss: 1.55e-06 | reg: 1.96e+00 \n",
      "step = 11800 | total loss: 1.94e-03 | train loss: 1.16e-06 | reg: 1.94e+00 \n",
      "step = 12000 | total loss: 1.89e-03 | train loss: 1.24e-06 | reg: 1.89e+00 \n",
      "step = 12200 | total loss: 1.88e-03 | train loss: 1.22e-06 | reg: 1.88e+00 \n",
      "step = 12400 | total loss: 1.88e-03 | train loss: 1.22e-06 | reg: 1.88e+00 \n",
      "step = 12600 | total loss: 1.88e-03 | train loss: 1.14e-06 | reg: 1.88e+00 \n",
      "step = 12800 | total loss: 1.88e-03 | train loss: 1.11e-06 | reg: 1.88e+00 \n",
      "step = 13000 | total loss: 1.88e-03 | train loss: 1.68e-06 | reg: 1.88e+00 \n",
      "step = 13200 | total loss: 1.88e-03 | train loss: 1.36e-06 | reg: 1.87e+00 \n",
      "step = 13400 | total loss: 1.88e-03 | train loss: 1.15e-06 | reg: 1.88e+00 \n",
      "step = 13600 | total loss: 1.88e-03 | train loss: 1.34e-06 | reg: 1.88e+00 \n",
      "step = 13800 | total loss: 1.88e-03 | train loss: 1.52e-06 | reg: 1.87e+00 \n",
      "step = 14000 | total loss: 1.88e-03 | train loss: 1.37e-06 | reg: 1.87e+00 \n",
      "step = 14200 | total loss: 1.88e-03 | train loss: 1.37e-06 | reg: 1.88e+00 \n",
      "step = 14400 | total loss: 1.87e-03 | train loss: 1.41e-06 | reg: 1.87e+00 \n",
      "step = 14600 | total loss: 1.88e-03 | train loss: 1.32e-06 | reg: 1.87e+00 \n",
      "step = 14800 | total loss: 1.88e-03 | train loss: 1.46e-06 | reg: 1.87e+00 \n",
      "step = 15000 | total loss: 1.88e-03 | train loss: 1.21e-06 | reg: 1.87e+00 \n",
      "step = 15200 | total loss: 1.87e-03 | train loss: 1.29e-06 | reg: 1.87e+00 \n",
      "step = 15400 | total loss: 1.88e-03 | train loss: 1.45e-06 | reg: 1.87e+00 \n",
      "step = 15600 | total loss: 1.87e-03 | train loss: 1.37e-06 | reg: 1.87e+00 \n",
      "step = 15800 | total loss: 1.88e-03 | train loss: 1.46e-06 | reg: 1.87e+00 \n",
      "step = 16000 | total loss: 1.88e-03 | train loss: 1.38e-06 | reg: 1.88e+00 \n",
      "step = 16200 | total loss: 1.88e-03 | train loss: 1.27e-06 | reg: 1.88e+00 \n",
      "step = 16400 | total loss: 1.88e-03 | train loss: 1.32e-06 | reg: 1.88e+00 \n",
      "step = 16600 | total loss: 1.87e-03 | train loss: 1.33e-06 | reg: 1.87e+00 \n",
      "step = 16800 | total loss: 1.88e-03 | train loss: 1.06e-06 | reg: 1.88e+00 \n",
      "step = 17000 | total loss: 1.88e-03 | train loss: 1.56e-06 | reg: 1.88e+00 \n",
      "step = 17200 | total loss: 1.88e-03 | train loss: 1.74e-06 | reg: 1.88e+00 \n",
      "step = 17400 | total loss: 1.88e-03 | train loss: 1.04e-06 | reg: 1.87e+00 \n",
      "step = 17600 | total loss: 1.87e-03 | train loss: 1.55e-06 | reg: 1.87e+00 \n",
      "step = 17800 | total loss: 1.88e-03 | train loss: 7.08e-07 | reg: 1.87e+00 \n",
      "step = 18000 | total loss: 1.88e-03 | train loss: 1.49e-06 | reg: 1.88e+00 \n",
      "step = 18200 | total loss: 1.88e-03 | train loss: 1.50e-06 | reg: 1.88e+00 \n",
      "step = 18400 | total loss: 1.88e-03 | train loss: 1.20e-06 | reg: 1.87e+00 \n",
      "step = 18600 | total loss: 1.88e-03 | train loss: 1.16e-06 | reg: 1.88e+00 \n",
      "step = 18800 | total loss: 1.88e-03 | train loss: 1.57e-06 | reg: 1.88e+00 \n",
      "step = 19000 | total loss: 1.87e-03 | train loss: 1.32e-06 | reg: 1.87e+00 \n",
      "step = 19200 | total loss: 1.87e-03 | train loss: 1.15e-06 | reg: 1.87e+00 \n",
      "step = 19400 | total loss: 1.88e-03 | train loss: 1.42e-06 | reg: 1.87e+00 \n",
      "step = 19600 | total loss: 1.88e-03 | train loss: 1.15e-06 | reg: 1.87e+00 \n",
      "step = 19800 | total loss: 1.88e-03 | train loss: 1.36e-06 | reg: 1.88e+00 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# create model\n",
    "seed = 3\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# d_in = n_control + n_task\n",
    "# d_out = 1\n",
    "# shp = [d_in, 20, d_out]\n",
    "\n",
    "shp = [6, 50, 6]\n",
    "\n",
    "model = BioMLP(shp=shp)\n",
    "\n",
    "n_task = 12\n",
    "n_output = 6\n",
    "in_formulas =  [\"T%d\"%jj for jj in np.arange(n_task)]\n",
    "out_formulas = [r\"out\" for jj in np.arange(n_output)]\n",
    "\n",
    "# training hyper-parameters\n",
    "# train_type = 1; no L1\n",
    "# train_type = 2; L1\n",
    "# train_type = 3: L1 + Local\n",
    "# train_type = 4: L1 + Swap\n",
    "# train_type = 5: L1 + Local + Swap\n",
    "train_type = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.0)\n",
    "log = 200\n",
    "lamb = 0 if train_type==1 else 0.001\n",
    "swap_log = 200 if train_type >= 4 else float('inf')\n",
    "weight_factor = 1. if train_type == 3 or train_type == 5 else 0.\n",
    "plot_log = 2000\n",
    "steps = 20000\n",
    " \n",
    "    \n",
    "frame_dir = 'frames'\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "for step in range(steps):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pred  = model(inputs)\n",
    "    loss = torch.mean((pred-labels)**2)\n",
    "    reg = model.get_cc(bias_penalize=True, weight_factor=weight_factor)\n",
    "    \n",
    "    total_loss = loss + lamb*reg\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % log == 0:\n",
    "        print(\"step = %d | total loss: %.2e | train loss: %.2e | reg: %.2e \"%(step, total_loss.detach().numpy(), loss.detach().numpy(), reg.detach().numpy()))\n",
    "    \n",
    "    if (step+1) % swap_log == 0:\n",
    "        model.relocate()\n",
    "\n",
    "    if step % plot_log == 0:\n",
    "        model.plot()\n",
    "        fontsize = 8\n",
    "        for j in range(shp[0]):\n",
    "            plt.text(0.5/(2*shp[0])+j/shp[0], -0.03, in_formulas[model.in_perm[j].long()], fontsize=fontsize)\n",
    "\n",
    "        for j in range(shp[-1]):\n",
    "            plt.text(1/(2*shp[-1])+j/shp[-1]-0.05, 0.1*(len(shp)-1)+0.01, out_formulas[model.out_perm[j].long()], fontsize=fontsize)\n",
    "\n",
    "        # plt.show()\n",
    "        \n",
    "        \n",
    "        frame_filename = os.path.join(frame_dir, f'frame_{step}.png')\n",
    "        plt.savefig(frame_filename)\n",
    "        plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "259ba206-eb67-4b35-adf0-56106e28c7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/slurm_tmp/24109722.0.0/ipykernel_2626964/2711147112.py:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  images = [imageio.imread(os.path.join(frame_dir, f'frame_{step}.png')) for step in range(0, steps, plot_log)]\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "images = [imageio.imread(os.path.join(frame_dir, f'frame_{step}.png')) for step in range(0, steps, plot_log)]\n",
    "imageio.mimsave('animation.gif', images, duration=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6fd019-6568-41b1-bdf0-bc0e4bc202f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
