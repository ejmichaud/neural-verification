{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e0fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c34c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_verification import MLP, MLPConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7493cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000 # number of samples\n",
    "n = 2 # number of numbers per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb9b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0,1,size=(N, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95112d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in range(N):\n",
    "    y.append([X[i][0]*X[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd67c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d329ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1258, -1.1524],\n",
       "        [-0.2506, -0.4339],\n",
       "        [ 0.8487,  0.6920],\n",
       "        ...,\n",
       "        [ 0.3877,  1.0991],\n",
       "        [ 0.4403,  1.6217],\n",
       "        [ 2.1894, -0.4871]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b06b3b5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2974],\n",
       "        [ 0.1087],\n",
       "        [ 0.5873],\n",
       "        ...,\n",
       "        [ 0.4261],\n",
       "        [ 0.7141],\n",
       "        [-1.0665]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3340925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d50cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342c6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b943c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d):\n",
    "    print(f\"Training model with {d} nodes in hidden layer\")\n",
    "    model = MLP(MLPConfig(width=d))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=250, gamma=0.8)\n",
    "    num_epochs = 10000\n",
    "    training_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        epochloss = loss_fn(y_pred, y)\n",
    "        training_loss.append(epochloss.item())\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Learning Rate: {optimizer.param_groups[0][\"lr\"]}, Loss: {epochloss}')\n",
    "    plt.plot(training_loss, label=f'train_loss (d = {d})')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show\n",
    "    plt.savefig(f'multiply-results/multiply{d}-5.png')\n",
    "    torch.save(model.state_dict(), f\"model{d}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c0ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [10, 15, 25, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17373cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 10 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.006456615403294563\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.006015562452375889\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.004806334152817726\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.003131282050162554\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.0019444251665845513\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.0016664274735376239\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.007425518706440926\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0012765012215822935\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0030190779361873865\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.0042333886958658695\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0007466765819117427\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.005313146393746138\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0015556892612949014\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0002804457035381347\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0010657014790922403\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.00018395448569208384\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.0005515493103303015\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00012278117355890572\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 9.014169336296618e-05\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.00020690535893663764\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 5.329734267434105e-05\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 5.086686724098399e-05\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 3.678910434246063e-05\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 8.531452476745471e-05\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 3.469453804427758e-05\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 7.678131805732846e-05\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 1.8025240933638997e-05\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 8.862904360285029e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 3.690264929900877e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 3.983180067734793e-05\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0001527743152109906\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 3.755948637262918e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 2.0126426534261554e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 3.131561243208125e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 5.073444754088996e-06\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 1.6028539903345518e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 8.448302651231643e-06\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 6.32578894510516e-06\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 7.2703160185483284e-06\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 7.773244760755915e-06\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 3.8103103179309983e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 8.581172551203053e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 3.6133467347099213e-06\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 4.160711614531465e-05\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 6.508919796033297e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 1.2866940778621938e-05\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 5.5966211220948026e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 4.9058003241952974e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 1.8267052155351848e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 2.846189318006509e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 3.0475857784040272e-06\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 3.82985081159859e-06\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 1.5248840554704657e-06\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 3.3970102322200546e-06\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 1.6489664176333463e-06\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 1.6875915207492653e-06\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 1.691540319370688e-06\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 1.474543523727334e-06\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 1.1483449497973197e-06\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 1.1358287110851961e-06\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 1.0087393320645788e-06\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 1.3556231124312035e-06\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 8.386062404497352e-07\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 7.507530312977906e-07\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 9.400218914379366e-07\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 1.0687397207220783e-06\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 8.513665079590282e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 8.625021905572794e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 8.31742227092036e-07\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 6.713896141263831e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 1.4549774505212554e-06\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 7.033755196061975e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 6.944739538994327e-07\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 6.147253088784055e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 7.52719643060118e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 7.913614581411821e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 6.235414389266225e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 7.810710371813911e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 6.644629593210993e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 6.086359007895226e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 6.072301061976759e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 7.02573231592396e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 6.364967930494458e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 6.381775961017411e-07\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 6.023802257004718e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 5.60408750516217e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 5.827979521200177e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 5.819842954224441e-07\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 5.822250841447385e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 5.722038736166724e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 5.559720079872932e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 5.555358484343742e-07\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 5.376328431339061e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 5.619701823889045e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 5.434708896245866e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 5.545286398955795e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 5.6394287639705e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 5.533605076379899e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 5.311064228408213e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 5.378798277888563e-07\n",
      "Training model with 15 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.006620096042752266\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.003924510441720486\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.004395985975861549\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.0012621936621144414\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.0033959313295781612\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.0023357293102890253\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.002321717794984579\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0014345935778692365\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0036314346361905336\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.0014513180358335376\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0003140492190141231\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0006513878470286727\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0009147403761744499\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.006187984254211187\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0006189506384544075\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.0006553138955496252\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.00018893428205046803\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00017615495016798377\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.0006333916680887341\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.0003866031765937805\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.0001373876875732094\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00018281197117175907\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.00036220994661562145\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 4.029319097753614e-05\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 7.393410487566143e-05\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 8.397430065087974e-05\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.00011524488945724443\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 7.641947013325989e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 7.76318265707232e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 4.342952524893917e-05\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 5.564099410548806e-05\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 3.0974362744018435e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 5.425839844974689e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 7.636807822564151e-06\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 1.4490711691905744e-05\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 4.495486791711301e-06\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 1.372126007481711e-05\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 1.3008317182539031e-05\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 4.899312989437021e-05\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 3.394406576262554e-06\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 4.194210305286106e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 5.30661100128782e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 4.6624568312836345e-06\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 1.7472699482823373e-06\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 2.688507947823382e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 4.3598511183517985e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 3.1950510219758144e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 4.878497293248074e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 1.306797230427037e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 5.0914263738377485e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 7.648130122106522e-06\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 1.628433892619796e-06\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 8.111033480417973e-07\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 9.361278330288769e-07\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 7.248346491905977e-07\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 8.899546060092689e-07\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 8.755006888350181e-07\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 5.099307145428611e-07\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 5.118993158248486e-07\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 5.920240369050589e-07\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 8.009960765775759e-07\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 4.757332590088481e-07\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 8.985459771793103e-07\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 5.439671895146603e-07\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 8.490879963574116e-07\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 8.553722636861494e-07\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 4.64091158391966e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 5.26839357917197e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 3.74766926825032e-07\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 4.417436798576091e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 3.113479749572434e-07\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 5.138129495207977e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 3.176924394665548e-07\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 3.193868849393766e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 2.922263320215279e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 3.5577753010329616e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 2.8663137641160574e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 3.3306352520412474e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 3.4326799891459814e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.4821891858882736e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.9509433236162295e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.7992641093987913e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 2.918181962741073e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 2.6841769340535393e-07\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.533965073325817e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.5861518793135474e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.670658716397156e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 2.562680379014637e-07\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 2.4879651050468965e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.523046589431033e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.443223934278649e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.5081379817493143e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 2.5006698933793814e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 2.4351609795303375e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 2.4012229005165864e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 2.426210699013609e-07\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 2.4638771378704405e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 2.364758699968661e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 2.317840994692233e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 2.3628173551060172e-07\n",
      "Training model with 25 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.009283375926315784\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.010214120149612427\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.008220012299716473\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.0027982862666249275\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.003099306020885706\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.011474632658064365\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.0033627499360591173\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.007565466221421957\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0016424267087131739\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.002790922997519374\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0006639917846769094\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0004784280899912119\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0005806760746054351\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0018038953421637416\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0012597345048561692\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.000126293656649068\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.0014920789981260896\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00011328471737215295\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.000233795159147121\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 9.120209870161489e-05\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 3.307791848783381e-05\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00013923677033744752\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.0001109529475797899\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 7.458013715222478e-05\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.00020983751164749265\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 1.9362563762115315e-05\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 3.453778845141642e-05\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 9.344837599201128e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 2.2763777451473288e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0001700705470284447\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 1.374241583107505e-05\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 1.6937787222559564e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 2.5363782697240822e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 1.6534275346202776e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 1.169432289316319e-05\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 2.3671031158301048e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 1.0481736353540327e-05\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 5.382599738368299e-06\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 4.9647264859231655e-06\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 2.732238726821379e-06\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 2.2684000668959925e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 1.7670612351139425e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 2.902031837948016e-06\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 9.059745025297161e-06\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 1.4765104197067558e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 3.437926579863415e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 1.2828509170503821e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 1.3274125194584485e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 1.8955538507725578e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 1.5092440435182652e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 1.1099738230768708e-06\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 1.858000246102165e-06\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 6.902097311467514e-07\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 9.616959459890495e-07\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 5.127936333337857e-07\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 1.1056635003114934e-06\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 1.3447375977193587e-06\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 4.752271252073115e-07\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 6.82687925745995e-07\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 3.108767998583062e-07\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 1.1298603794784867e-06\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 5.391107151808683e-07\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 3.689198706524621e-07\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 4.204388233119971e-07\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 3.2508714298273844e-07\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 2.2249831488352356e-07\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 3.4902825518656755e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 2.3913366931083146e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 3.0241187687352067e-07\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 3.180297198923654e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 1.8945637236811308e-07\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 2.56429842693251e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 2.3016505679152033e-07\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 2.9201669349276926e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 2.0656131027863012e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 1.961493865110242e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 1.8740445284493035e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 1.8607958907068678e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 2.0644972664740635e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.351450802962063e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.8197810902620404e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.7045212530319986e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.7789845685456385e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.6376586131627846e-07\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.6829103799409495e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.7738769031439006e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.5972926803442533e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 1.8664509582322353e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 1.5780689466282638e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.575822921040526e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.5113764106899907e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.5437569800269557e-07\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.7081566738852416e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.530876545530191e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.5084511062468664e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.4984144058871607e-07\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.5055728397328494e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.476919777587682e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.48592945947712e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 1.5242181916619302e-07\n",
      "Training model with 50 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.003799459431320429\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.005390163976699114\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.00211321166716516\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.004383997526019812\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.08357607573270798\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.00393403647467494\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.0017516511725261807\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.006672320421785116\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0022821947932243347\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.002945925807580352\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0011514290235936642\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0022637953516095877\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0005587749183177948\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.004229406360536814\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0011873462935909629\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.00039384711999446154\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.00019380157755222172\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00025607828865759075\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.00018926315533462912\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.00032626494066789746\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.0001041828072629869\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00021084651234559715\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.00016061919450294226\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 5.397065979195759e-05\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 9.584431973053142e-05\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 5.089597834739834e-05\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 3.615016976254992e-05\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 1.553007314214483e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 3.4885280911112204e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 2.5715526135172695e-05\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 4.925871326122433e-05\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 2.9074657504679635e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 3.845473111141473e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 3.072906838497147e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 3.915815977961756e-05\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 1.4728921996720601e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 6.5486938183312304e-06\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 9.664067874837201e-06\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 2.451991349516902e-06\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 9.563073945173528e-06\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 3.2428197300760075e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 1.7498028910267749e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 3.53252448803687e-06\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 8.165783583535813e-06\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 1.1973878599746968e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 4.0408835957350675e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 1.3101844160701148e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 1.7029540231305873e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 1.582728714311088e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 1.06312461412017e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 9.795553523872513e-07\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 6.722867169628444e-07\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 7.878037990849407e-07\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 1.1065750413763453e-06\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 1.6297677802867838e-06\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 7.882015893301286e-07\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 2.5021456622198457e-06\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 5.251696961749985e-07\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 6.541177413055266e-07\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 6.636109333157947e-07\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 9.353212249152421e-07\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 5.222084951128636e-07\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 2.678210648809909e-07\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 3.788248079672485e-07\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 3.2594286381026905e-07\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 3.273979132245586e-07\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 2.7259207513452566e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 3.558064349817869e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 3.0537142947650864e-07\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 2.32047725035045e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 3.834745996300626e-07\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 3.5634690220831544e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 3.5498769079822523e-07\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 2.544850303820567e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 2.1374614789237967e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 2.842234891886619e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 2.7295357085677097e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 2.684666355889931e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 2.3606685317645315e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.3275448768345086e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.2322582537981361e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.2557361489816685e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.9967831121903146e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.9760049951855763e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.1082888679302414e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.1814496165006858e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.204595404009524e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 2.0097955655273836e-07\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 2.0019747637434193e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.132011047706328e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.9599220024701935e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.1311804232482245e-07\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.8710048266257218e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.9503329440340167e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.9632842906958103e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 2.055646177723247e-07\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.8198397810920142e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.8076060825933382e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.8620107766764704e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 1.8219061814761517e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for d in ds:\n",
    "    train(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e0e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
