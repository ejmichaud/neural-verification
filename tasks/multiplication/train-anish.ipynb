{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e0fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1000ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=2, w=2, depth=2, shp=None):\n",
    "        super(MLP, self).__init__()\n",
    "        if shp == None:\n",
    "            shp = [in_dim] + [w]*(depth-1) + [out_dim]\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.depth = depth\n",
    "                 \n",
    "        else:\n",
    "            self.in_dim = shp[0]\n",
    "            self.out_dim = shp[-1]\n",
    "            self.depth = len(shp) - 1\n",
    "        linear_list = []\n",
    "        for i in range(self.depth):\n",
    "            linear_list.append(nn.Linear(shp[i], shp[i+1]))\n",
    "        self.linears = nn.ModuleList(linear_list)\n",
    "        self.shp = shp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input shape = (batch_size, input_dim)\n",
    "        # define activation here\n",
    "        #f = lambda x: 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "        f = torch.nn.SiLU()\n",
    "        for i in range(self.depth-1):\n",
    "            x = f(self.linears[i](x))\n",
    "        x = self.linears[-1](x)\n",
    "        # output shape = (batch_size, output_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7493cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000 # number of samples\n",
    "n = 2 # input numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb9b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0,1,size=(N, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95112d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "for i in range(N):\n",
    "    y.append([X[i][0]*X[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd67c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d329ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5386,  0.1047],\n",
       "        [-0.4411,  0.6766],\n",
       "        [-0.2474, -1.4252],\n",
       "        ...,\n",
       "        [-1.3155,  1.2652],\n",
       "        [ 1.8891, -0.7887],\n",
       "        [ 0.4402,  0.5206]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b06b3b5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1610],\n",
       "        [-0.2985],\n",
       "        [ 0.3526],\n",
       "        ...,\n",
       "        [-1.6644],\n",
       "        [-1.4899],\n",
       "        [ 0.2292]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3340925",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d50cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "342c6d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b943c4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d):\n",
    "    print(f\"Training model with {d} nodes in hidden layer\")\n",
    "    shp = [2, d, 1]\n",
    "    model = MLP(shp=shp)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=250, gamma=0.8)\n",
    "    num_epochs = 10000\n",
    "    training_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        epochloss = loss_fn(y_pred, y)\n",
    "        training_loss.append(epochloss.item())\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Learning Rate: {optimizer.param_groups[0][\"lr\"]}, Loss: {epochloss}')\n",
    "    plt.plot(training_loss, label=f'train_loss (d = {d})')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show\n",
    "    plt.savefig(f'multiply-results/multiply{d}-4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7c0ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 25, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17373cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 2 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.11245257407426834\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.5445375442504883\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.5534116625785828\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.5273420214653015\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.551365077495575\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.5341973900794983\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.5542284250259399\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.5228914022445679\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.5262500643730164\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.5419334769248962\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.5215558409690857\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.5228440165519714\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.521445095539093\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.5231857895851135\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.5259758234024048\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.5220682621002197\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.5258162021636963\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.5270929336547852\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.5298072695732117\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.5229833722114563\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.523299515247345\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.5215006470680237\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.5207154750823975\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 0.522239625453949\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.5202637910842896\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.5209773182868958\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.5207259058952332\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 0.5191991329193115\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 0.5208529233932495\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 0.5218389630317688\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 0.522086501121521\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 0.5188013315200806\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 0.5191283226013184\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 0.5185118913650513\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 0.5193007588386536\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 0.5184492468833923\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 0.5180370807647705\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 0.5189002156257629\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 0.517932653427124\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 0.5178670883178711\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 0.5180535912513733\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 0.5179018378257751\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 0.5177749991416931\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 0.5180859565734863\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 0.5179111361503601\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 0.5179361701011658\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 0.5185192823410034\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 0.517774760723114\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 0.5176764130592346\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 0.5177499055862427\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 0.5177990198135376\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 0.5178536772727966\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 0.5176529884338379\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 0.5176789164543152\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 0.5176433324813843\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 0.5176472663879395\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 0.5176350474357605\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 0.5176230072975159\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 0.5176329612731934\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 0.5176053047180176\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 0.5176072120666504\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 0.5176059603691101\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 0.5176013708114624\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 0.5175951719284058\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 0.5175937414169312\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 0.5175966024398804\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 0.5175988078117371\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 0.517590343952179\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 0.5175938606262207\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 0.5175880789756775\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 0.5175859332084656\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 0.5175898671150208\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 0.517585813999176\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 0.5175843834877014\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 0.517583966255188\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 0.5175817608833313\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 0.5175825357437134\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 0.5175808668136597\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 0.5175790190696716\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 0.517578661441803\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 0.5175779461860657\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 0.5175777673721313\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 0.5175768733024597\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 0.517577588558197\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 0.5175778269767761\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 0.5175789594650269\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 0.5175755620002747\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 0.517575204372406\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 0.5175749063491821\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 0.5175752639770508\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 0.5175744891166687\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 0.5175747275352478\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 0.5175742506980896\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 0.5175740718841553\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 0.5175746083259583\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 0.5175740122795105\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 0.5175736546516418\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 0.5175736546516418\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 0.5175737142562866\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 0.5175732374191284\n",
      "Training model with 3 nodes in hidden layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.015731491148471832\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.023189911618828773\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.013658099807798862\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.022017402574419975\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.010827131569385529\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.016323089599609375\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.02443385310471058\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.010582584887742996\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.014685772359371185\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.008963484317064285\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.009245107881724834\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.008392945863306522\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.006017948035150766\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.004521642811596394\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.03176906332373619\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.006126870401203632\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.00893961451947689\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.003468594281002879\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.005355504807084799\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.003923221956938505\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.008680053986608982\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.006298911292105913\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.0030087248887866735\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 0.00453775841742754\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.0036135991103947163\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.004024646244943142\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.005921434611082077\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 0.0026106464210897684\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 0.0030911213252693415\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0020867809653282166\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 0.002162560820579529\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 0.001883345772512257\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 0.002837816486135125\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 0.0017049291636794806\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 0.02794063650071621\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 0.0019895629957318306\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 0.0020865164697170258\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 0.0027185254730284214\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 0.0018940545851364732\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 0.002028697868809104\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 0.0018545170314610004\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 0.001957471715286374\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 0.0015872562071308494\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 0.0016936338506639004\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 0.0017789541743695736\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 0.0014766628155484796\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 0.0021173374261707067\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 0.001493360148742795\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 0.0014904836425557733\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 0.0015742462128400803\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 0.0016324069583788514\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 0.0015186110977083445\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 0.00141484837513417\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 0.0014378703199326992\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 0.0013986375415697694\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 0.0019298774423077703\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 0.0015022876905277371\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 0.001367393764667213\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 0.0013825125060975552\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 0.0013138488866388798\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 0.001299713272601366\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 0.0013326320331543684\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 0.0013727890327572823\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 0.0013408107915893197\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 0.0013107808772474527\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 0.0013892679708078504\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 0.0012741301907226443\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 0.0012895760592073202\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 0.0013352797832340002\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 0.0012876680120825768\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 0.0012582984054461122\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 0.0012786100851371884\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 0.0012685259571298957\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 0.0012587675591930747\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 0.0012695787008851767\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 0.0012581262271851301\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 0.001257230993360281\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 0.0012506237253546715\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 0.001267358660697937\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 0.001250609871931374\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 0.0012505657505244017\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 0.0012676115147769451\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 0.001242130296304822\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 0.0012419922277331352\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 0.0012535365531221032\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 0.001233275979757309\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 0.0012327434960752726\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 0.001236051321029663\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 0.0012325793504714966\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 0.0012440907303243876\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 0.0012304048286750913\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 0.001232682610861957\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 0.0012312395265325904\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 0.0012305075069889426\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 0.001229479443281889\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 0.001228349981829524\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 0.001229592366144061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 0.0012282026000320911\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 0.0012271917657926679\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 0.0012269633589312434\n",
      "Training model with 4 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.004291970282793045\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.0031176810152828693\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.001833855640143156\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.0017716396832838655\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.0053563192486763\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.0020185341127216816\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.0016376577550545335\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0011500660330057144\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0027321286033838987\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.006863206624984741\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0010053254663944244\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0020939349196851254\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.004130516666918993\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0007421292830258608\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0010852259583771229\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.0014641436282545328\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.00035647395998239517\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.000305794645100832\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.0004366746870800853\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.00041610232437960804\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.011575834825634956\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00014057020598556846\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.00015179092588368803\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 0.0003657042980194092\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.000602161861024797\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.00045636753202416003\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.00045660423347726464\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 0.0003150632546748966\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 0.00017184139869641513\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0002507485041860491\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0002668278757482767\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0015936645213514566\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 0.00010767256026156247\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 9.381376730743796e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 7.913225999800488e-05\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 4.8171772505156696e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 0.00014095257211010903\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 4.216598972561769e-05\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 4.461140633793548e-05\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 0.00012277755013201386\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 4.5013875933364034e-05\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 9.6556541393511e-05\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 0.00010690392082324252\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 4.8226149374386296e-05\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 8.676875586388633e-05\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 7.391269173240289e-05\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 3.383315197424963e-05\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 4.7572117182426155e-05\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 2.7866590244229883e-05\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 0.0002760515781119466\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 3.185509194736369e-05\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 3.212991214240901e-05\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 4.5258111640578136e-05\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 2.4340943127754144e-05\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 2.7205189326195978e-05\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 2.6321766199544072e-05\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 2.830586709023919e-05\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 2.5524303055135533e-05\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 2.33045775530627e-05\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 3.0895869713276625e-05\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 3.749630559468642e-05\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 2.268812204420101e-05\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 2.6121342671103776e-05\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 2.336836223548744e-05\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 3.309520616312511e-05\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 2.1724950784118846e-05\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 2.037020203715656e-05\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 2.0379875422804616e-05\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 2.0015293557662517e-05\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 2.1605732399621047e-05\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 2.252003105240874e-05\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 2.0044348275405355e-05\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 1.985510971280746e-05\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 1.946390329976566e-05\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 1.9381990568945184e-05\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 2.205761666118633e-05\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 2.113463960995432e-05\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 1.9614601114881225e-05\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 1.850135413405951e-05\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.922464980452787e-05\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.886596146505326e-05\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.91512554010842e-05\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.8504151739762165e-05\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.8448858099873178e-05\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.8345659555052407e-05\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.822796002670657e-05\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.8186770830652677e-05\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 1.8343371266382746e-05\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 1.822789818106685e-05\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.8168580936617218e-05\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.7988868421525694e-05\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.8090358935296535e-05\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.8008971892413683e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.7930742615135387e-05\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.7918744561029598e-05\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.7830750948633067e-05\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.8156302758143283e-05\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.782203617040068e-05\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.7799164197640494e-05\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 1.777195939212106e-05\n",
      "Training model with 5 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.017835015431046486\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.00886065699160099\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.005485361907631159\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.002191440900787711\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.0024480067659169436\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.04147741198539734\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.0017138412222266197\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.002674488350749016\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.004096013028174639\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.003398873843252659\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.002648744499310851\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.005971984006464481\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0013889414258301258\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0007024084916338325\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.001020933617837727\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.0009774885838851333\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.0010970719158649445\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.0013410873943939805\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.0004463779041543603\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.00041675320244394243\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.0006610947311855853\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.0002877421211451292\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.00019503774819895625\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 0.0011331434361636639\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.0005902965785935521\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.00040553841972723603\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.0004730008658953011\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 0.000329848233377561\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 0.00014977247337810695\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0003241010708734393\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0006577262538485229\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 0.00030215844162739813\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 0.0001703305315459147\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 0.00013725983444601297\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 0.0002562048612162471\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 0.00027628650423139334\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 9.701791714178398e-05\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 0.0001781820465112105\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 0.0001175382494693622\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 0.000124380923807621\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 9.109998063649982e-05\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 7.021336205070838e-05\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 6.843716255389154e-05\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 9.86083978204988e-05\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 6.95858325343579e-05\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 6.294759805314243e-05\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 0.00016061498899944127\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 5.92292781220749e-05\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 5.265285290079191e-05\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 5.3239309636410326e-05\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 9.535051503917202e-05\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 5.8740282838698477e-05\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 5.095585584058426e-05\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 4.821019319933839e-05\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 4.618074308382347e-05\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 4.9624344683252275e-05\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 4.631073898053728e-05\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 4.362207619124092e-05\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 3.748898234334774e-05\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 3.8172409404069185e-05\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 6.465840124292299e-05\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 3.665435724542476e-05\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 4.2522791773080826e-05\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 3.4564185625640675e-05\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 3.70080306311138e-05\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 5.3836058214074e-05\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 3.371706770849414e-05\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 3.9389673474943265e-05\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 3.7297893868526444e-05\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 3.428324271226302e-05\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 3.53850846295245e-05\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 3.2621923310216516e-05\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 3.226202170480974e-05\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 3.4147822589147836e-05\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 3.6103843740420416e-05\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 3.2907893910305575e-05\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 3.694704355439171e-05\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 3.1642142857890576e-05\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 3.106231088167988e-05\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.125324656139128e-05\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.2507807190995663e-05\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.0724033422302455e-05\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 3.089136953349225e-05\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 3.08411217702087e-05\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 3.1234361813403666e-05\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 3.04362692986615e-05\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 3.0685143428854644e-05\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 3.0674724257551134e-05\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 3.0445891752606258e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.0369434171007015e-05\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.0802788387518376e-05\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.0178311135387048e-05\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 3.021467273356393e-05\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 3.078129157074727e-05\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.042362368432805e-05\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.003407982760109e-05\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.018427742063068e-05\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 3.0132088795653544e-05\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 3.05431240121834e-05\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 3.0457253160420805e-05\n",
      "Training model with 6 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.0028700632974505424\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.0030482399743050337\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.0022740226704627275\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.003525764914229512\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.005028534680604935\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.001750854542478919\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.003815706353634596\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0011078206589445472\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.006230690982192755\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.0005146004259586334\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0009703148971311748\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.00041251411312259734\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0004560390079859644\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.00026899680960923433\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.00045398506335914135\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.00107212970033288\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.0004889289848506451\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00028359584393911064\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.00028389281942509115\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.00033949018688872457\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.0004909854615107179\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00016589260485488921\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.0004286925832275301\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 0.0002539521374274045\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.004250391386449337\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.0002331979339942336\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.0003003383462782949\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 0.0001516483025625348\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 7.154288323363289e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 0.000183382275281474\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0004966194974258542\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 5.859172233613208e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 8.347173570655286e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 3.770670446101576e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 4.495117536862381e-05\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 0.00019193252956029028\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 4.361658648122102e-05\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 3.9698832551948726e-05\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 0.00010258708061883226\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 4.740611257147975e-05\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 4.457302566152066e-05\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 6.943273183424026e-05\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 3.0131113817333244e-05\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 0.00011131470091640949\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 7.372038089670241e-05\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 2.50531047640834e-05\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 4.196127338218503e-05\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 2.8727650715154596e-05\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 3.336026566103101e-05\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 2.4718970962567255e-05\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 3.150158954667859e-05\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 2.249499266326893e-05\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 2.4613023924757726e-05\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 1.7443395336158574e-05\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 2.5142162485281006e-05\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 1.903562224470079e-05\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 2.3545973817817867e-05\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 1.7550299162394367e-05\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 1.9921872080885805e-05\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 2.8176069463370368e-05\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 1.529616019979585e-05\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 1.6232443158514798e-05\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 1.7396678231307305e-05\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 1.61866337293759e-05\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 1.8019893104792573e-05\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 1.8442022337694652e-05\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 1.5397892639157362e-05\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 1.7488166122348048e-05\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 1.5297511708922684e-05\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 1.6201673133764416e-05\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 1.477097157476237e-05\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 1.444524787075352e-05\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 1.382822847517673e-05\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 1.5926254491205327e-05\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 1.4732934687344823e-05\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 1.3635681170853786e-05\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 1.3811903954774607e-05\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 1.4160757018544246e-05\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 1.3519088497560006e-05\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.5230268218147103e-05\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.3476672393153422e-05\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.3845274224877357e-05\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.3397128896031063e-05\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.3226220289652701e-05\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.3750909602094907e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.3239179679658264e-05\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.3300174032337964e-05\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 1.3135751032677945e-05\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 1.3519844287657179e-05\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.314906057814369e-05\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.3210124052420724e-05\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.3222439520177431e-05\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.3076469258521684e-05\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.2975625395483803e-05\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.3026569831708912e-05\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.2920349945488852e-05\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.301924839935964e-05\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.2913079444842879e-05\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.2875884749519173e-05\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 1.290856562263798e-05\n",
      "Training model with 7 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.009784603491425514\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.0027549571823328733\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.0015720527153462172\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.0033735339529812336\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.005281343124806881\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.0021007403265684843\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.0020014795009046793\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0012924541952088475\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0019218437373638153\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.0005279240431264043\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0009078311850316823\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0008736203308217227\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.000790264515671879\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0006484946934506297\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0003875241382047534\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.0011171643855050206\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.004188441671431065\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00033274717861786485\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.00020175559620838612\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.0003523418272379786\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.00017350990674458444\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00019467940728645772\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 0.0002587050839792937\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 0.0009685430559329689\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.00020961492555215955\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.00021142193872947246\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.00011147360783070326\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 8.777181210462004e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 0.00014273839769884944\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 0.00020924284763168544\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 7.304811151698232e-05\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 3.9470007322961465e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 0.00013097609917167574\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 0.000137670140247792\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 0.0013258543331176043\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 0.00011367279512342066\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 3.205214670742862e-05\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 3.329703031340614e-05\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 6.061626845621504e-05\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 8.385466935578734e-05\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 4.997940777684562e-05\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 5.581691220868379e-05\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 5.357178451959044e-05\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 2.1387681044870988e-05\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 3.610948260757141e-05\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 2.6056828573928215e-05\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 2.2930047634872608e-05\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 1.896224966912996e-05\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 4.6021425077924505e-05\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 5.092053470434621e-05\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 2.0841171135543846e-05\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 3.1637202482670546e-05\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 2.0911460524075665e-05\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 1.849858927016612e-05\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 2.0784742446267046e-05\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 2.8644668418564834e-05\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 1.6937250620685518e-05\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 1.8419626940158196e-05\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 1.6219928511418402e-05\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 2.910344301199075e-05\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 1.7422105884179473e-05\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 1.554835580463987e-05\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 1.9905244698747993e-05\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 1.6487756511196494e-05\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 1.4903347619110718e-05\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 1.5275300029315986e-05\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 1.5441724826814607e-05\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 1.641734161239583e-05\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 1.4501407349598594e-05\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 1.6207650332944468e-05\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 1.3388152183324564e-05\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 1.4073180864215828e-05\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 1.3489364391716663e-05\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 1.3163213225197978e-05\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 1.3484358532878105e-05\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 1.2799710020772181e-05\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 1.4338194887386635e-05\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 1.2864676136814523e-05\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 1.2932284334965516e-05\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.294675803364953e-05\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.3764560208073817e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 1.2884360330644995e-05\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.2981546205992345e-05\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 1.2535921086964663e-05\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.2500044249463826e-05\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.269064250664087e-05\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 1.2425959539541509e-05\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 1.2416917343216483e-05\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 1.267871539312182e-05\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.2446102118701674e-05\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.2384963156364392e-05\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.2264448741916567e-05\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.2349001735856291e-05\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.2264162251085509e-05\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.2159557627455797e-05\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.2201709978398867e-05\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.2172039532742929e-05\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.2143083040427882e-05\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.2176843483757693e-05\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 1.2153674106230028e-05\n",
      "Training model with 8 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.001577426097355783\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.004617628641426563\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.0013245237059891224\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.00438236678019166\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.0010551187442615628\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.0013224430149421096\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.0006561001064255834\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0005669210804626346\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0026270223315805197\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.0021449769847095013\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0018364618299528956\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0009602083009667695\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0002450442116241902\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.00032771716360002756\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.00028847099747508764\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.0033088047057390213\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.0007991777965798974\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00032013014424592257\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.0006904324400238693\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.0003080167225562036\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.0001609830796951428\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00014739511243533343\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 7.077079499140382e-05\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 0.0008040765533223748\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.0001452509459340945\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.0001883457589428872\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.00014208161155693233\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 0.00010832736734300852\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 0.00022914513829164207\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 2.393461727479007e-05\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0001177955637103878\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 4.4071759475627914e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 1.4265805475588422e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 3.269353328505531e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 1.4316748092824128e-05\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 2.6452236852492206e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 1.4326681593956891e-05\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 1.183869881060673e-05\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 1.2013123523502145e-05\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 1.0091257536259945e-05\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 1.0812344953592401e-05\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 1.3206758922024164e-05\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 1.1650107808236498e-05\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 3.67485117749311e-05\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 1.0528749953664374e-05\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 9.283718100050464e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 1.9565497495932505e-05\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 9.445804607821628e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 9.799352483241819e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 8.942693057178985e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 1.139507003244944e-05\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 7.724839633738156e-06\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 7.093883141351398e-06\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 5.833372597408015e-06\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 6.21650178800337e-06\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 4.7906391955621075e-06\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 5.102742306917207e-06\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 1.0562895113253035e-05\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 4.9328518798574805e-06\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 5.495039204106433e-06\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 4.589048330672085e-06\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 5.752885499532567e-06\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 5.0257553994015325e-06\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 1.0228808605461381e-05\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 4.454887857718859e-06\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 4.416036972543225e-06\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 8.323974725499284e-06\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 4.524550604401156e-06\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 4.1069220060308e-06\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 4.3649306462612e-06\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 6.647466307185823e-06\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 4.019056632387219e-06\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 5.953622803644976e-06\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 3.954167368647177e-06\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 4.0589793570688926e-06\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 4.188432740193093e-06\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 4.078791789652314e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 3.832177299045725e-06\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 3.887332695740042e-06\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.845211267616833e-06\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.934928372473223e-06\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.7849954424018506e-06\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 3.8088694509497145e-06\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 3.7758768485218752e-06\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 3.835778898064746e-06\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 3.803858817263972e-06\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 3.828959961538203e-06\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 3.7547629290202167e-06\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 3.706970801431453e-06\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.7343063468142645e-06\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.6868439110548934e-06\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.688748847707757e-06\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 3.6682270092569524e-06\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 3.713888645506813e-06\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.683517434183159e-06\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.7407889976748265e-06\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.662489007183467e-06\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 3.661634764284827e-06\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 3.699698254422401e-06\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 3.6758199257747037e-06\n",
      "Training model with 9 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.0029214215464890003\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.0039699021726846695\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.006840791553258896\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.0031457352451980114\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.0026975031942129135\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.005955419968813658\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.0017434114124625921\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0018296908820047975\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.006012449041008949\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.0010029459372162819\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0005654487176798284\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0005841990932822227\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0005326311802491546\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.00024319053045473993\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0030136017594486475\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.0001556356146465987\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.00031182303791865706\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00046937461593188345\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.00010525599645916373\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 8.141432772390544e-05\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 7.847463712096214e-05\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.0001086442352971062\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 8.652297401567921e-05\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 1.8869390260078944e-05\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 3.5491189919412136e-05\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.00020183286687824875\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 4.387609442346729e-05\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 4.3413972889538854e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 2.04904172278475e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 1.143781810242217e-05\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 4.2837295040953904e-05\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 1.3613545888802037e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 7.146524694690015e-06\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 7.67204983276315e-06\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 7.866451596783008e-06\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 1.9911587514798157e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 1.7758926333044656e-05\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 3.485524302959675e-06\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 1.2916900232085027e-05\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 4.231666935083922e-06\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 3.7264621823851485e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 3.2249979540210916e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 2.642886101966724e-06\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 3.2696805192244938e-06\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 2.814620529534295e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 5.172909823158989e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 3.439144393269089e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 1.102577130041027e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 1.815447262742964e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 1.0335201068301103e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 1.996864284592448e-06\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 2.1638168163917726e-06\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 1.7416534774383763e-06\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 1.1774994845836773e-06\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 1.6091025827336125e-06\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 1.4090443301029154e-06\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 1.2971859177923761e-06\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 9.025047802424524e-07\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 1.424454353582405e-06\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 8.075519417616306e-07\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 9.634158004701021e-07\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 2.285912842125981e-06\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 9.106018410420802e-07\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 1.0462440513947513e-06\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 6.137657351246162e-07\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 5.834939997839683e-07\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 7.494258511542284e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 5.980504056424252e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 6.50524611955916e-07\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 5.11579798967432e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 5.247164835964213e-07\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 5.236950073594926e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 5.702381713490468e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 5.224570713835419e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 4.7104219902394107e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 5.186229259379616e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 4.7160932581391535e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 5.521440584743686e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 4.596434450832021e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.687008470227738e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.521587868566712e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.3052750697825104e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 4.4677014443550433e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 4.393042161154881e-07\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 5.556810265261447e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 4.5780129198647046e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 4.366557675439253e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 4.820569756702753e-07\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 4.15148292631784e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 4.576376682052796e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 4.0781466736916627e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 4.3625746570796764e-07\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 4.150436154759518e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 4.0370505871578644e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 4.2011131995423057e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 4.059229183894786e-07\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.9691823872090026e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 3.9698480236438627e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 3.951881524244527e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 4.0004726997722173e-07\n",
      "Training model with 10 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.010320138186216354\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.0032135143410414457\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.0012169037945568562\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.04449243098497391\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.004683072678744793\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.006762016098946333\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.0012242048978805542\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0036311561707407236\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.000770305166952312\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.001071125385351479\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0005165685433894396\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0028202561661601067\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.00021727432613261044\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0001363724732073024\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0009517399594187737\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.0001174898716271855\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 3.707396535901353e-05\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00024350789317395538\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.0001610488398000598\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.0002754000306595117\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.00012377719394862652\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00010785820632008836\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 9.120139293372631e-05\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 3.499358717817813e-05\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 3.709048542077653e-05\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 0.0001504689862485975\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 0.00016734865494072437\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 8.775195965426974e-06\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 1.27883313325583e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 8.789803359832149e-06\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 9.943630175257567e-06\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 1.6476789824082516e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 3.0924125894671306e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 1.5906958651612513e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 7.056944923533592e-06\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 4.776393325300887e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 3.2131410989677534e-06\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 4.567329142446397e-06\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 4.50883953817538e-06\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 1.3630855391966179e-05\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 7.410015314235352e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 2.1575310711341444e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 3.438641215325333e-06\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 2.4586393010395113e-06\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 7.388568064925494e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 8.373432137886994e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 2.1996845589455916e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 3.602865263019339e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 1.6542892353754723e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 3.4822717225324595e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 9.788062698135036e-07\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 5.168517873244127e-06\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 9.983441486838274e-07\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 1.694568823040754e-06\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 1.7613475620237296e-06\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 2.094946012221044e-06\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 3.0989499464340042e-06\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 1.027011080623197e-06\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 1.539302274977672e-06\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 8.732568517189065e-07\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 8.240505167123047e-07\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 5.916673444517073e-07\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 1.1479042996143107e-06\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 7.27444103176822e-07\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 1.067736889126536e-06\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 8.933022854762385e-07\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 7.345926746893383e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 6.107904368946038e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 5.871560233572382e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 8.770037425165356e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 6.037784601176099e-07\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 8.16491251498519e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 5.019511490900186e-07\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 6.615397296627634e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 5.547827299778874e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 4.670568785059004e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 4.987238639841962e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 4.809555775864283e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 5.368064535105077e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.93485231345403e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.950991865371179e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.930001864522637e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 4.403915170314576e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 4.4925027964382025e-07\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 4.616297246684553e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 4.5056901853968157e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 4.2909249486910994e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 4.6486081828334136e-07\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 4.743574208987411e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 4.374398372419819e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 4.207645929454884e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 4.304846754621394e-07\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 4.547477772121056e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 4.296349231935892e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 4.2163551938756427e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 4.144509375691996e-07\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 4.264838935341686e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 4.2289039470233547e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 4.133622724111774e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 4.3565174223658687e-07\n",
      "Training model with 15 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.010795864276587963\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.005084013566374779\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.004035450518131256\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.003274068469181657\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.00387627724558115\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.005714708939194679\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.005419753957539797\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.003963600378483534\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0008770063286647201\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.004533284343779087\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0010517110349610448\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.000858879357110709\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0003628131526056677\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0010235535446554422\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0009405762539245188\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.00023009240976534784\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.0001495585747761652\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 6.750774628017098e-05\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.0008092595380730927\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 6.688604480586946e-05\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 7.824895874364302e-05\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 2.366019725741353e-05\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 3.8098034565337e-05\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 4.7028526751091704e-05\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 0.0001866857783170417\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 1.779219564923551e-05\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 1.2569500540848821e-05\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 4.587528746924363e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 2.7755671908380464e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 0.0003996810410171747\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 1.7551492419443093e-05\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 3.577281677280553e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 7.61383535063942e-06\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 3.4073244023602456e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 7.811097020749003e-06\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 1.251389676326653e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 7.045684924378293e-06\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 1.1115246707049664e-05\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 5.2394671001820825e-06\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 5.11979033035459e-06\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 7.365406418102793e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 2.33248920267215e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 1.1361008546373341e-05\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 2.7054286420025164e-06\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 1.980338993234909e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 5.821429567731684e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 2.6305613118893234e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 2.882051376218442e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 1.6640943840684486e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 2.05663900487707e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 1.3262082347864634e-06\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 8.81568439581315e-07\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 3.577820507416618e-06\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 2.8574090720212553e-06\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 1.238778054357681e-06\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 2.4657847461639903e-06\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 1.0296300843037898e-06\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 9.826336508922395e-07\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 8.789393177721649e-07\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 1.4498433529297472e-06\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 1.1176980478921905e-06\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 8.051874260672776e-07\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 6.853407512608101e-07\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 6.983380558267527e-07\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 7.356326818808157e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 5.83652990826522e-07\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 4.974083367415005e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 6.066292712603172e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 5.40289420314366e-07\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 6.800513006055553e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 6.058711505829706e-07\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 4.219315030695725e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 5.567616767621075e-07\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 8.069030741353345e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 4.987234660802642e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 4.600431680046313e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 4.768917278852314e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 4.936244408781931e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 3.895056863711943e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.354435816367186e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.1523966842760274e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 4.086289209226379e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 4.515702300977864e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 4.4721755898535775e-07\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 4.557964530249592e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 4.128500847855321e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 4.049182962262421e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 4.0683002566765936e-07\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 4.095923031854909e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.782317037348548e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.7253937534842407e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 3.671613910682936e-07\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 3.643436343736539e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 3.7133571595404646e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.7475174963219615e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.806179620369221e-07\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 3.7508115724449453e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 3.686984655359993e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 3.701204320805118e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 3.617153367940773e-07\n",
      "Training model with 25 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.0028225863352417946\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.01152136642485857\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.0020574729423969984\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.001968157710507512\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.0030360554810613394\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.0028470936231315136\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.003385120304301381\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0018443624721840024\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0019338794518262148\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.0013587563298642635\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.00095739820972085\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.00038371034315787256\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.00029685001936741173\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0006258403300307691\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.001210671034641564\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.0010084914974868298\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.00021612830460071564\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.00010820350871654227\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.0007766963681206107\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.00028689581085927784\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 7.425130752380937e-05\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 0.00016765504551585764\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 7.110994192771614e-05\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 0.0012418318074196577\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 6.907594070071355e-05\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 2.4128836230374873e-05\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 1.971645360754337e-05\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 1.7938371456693858e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 3.8672020309604704e-05\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 1.160855117632309e-05\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 4.917854766972596e-06\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 1.172375323221786e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 2.1272502635838464e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 1.902785515994765e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 4.0212366002378985e-05\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 1.0424661013530567e-05\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 3.637888994489913e-06\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 1.576871181896422e-05\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 2.5671759431133978e-05\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 5.685433279722929e-06\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 1.2697964848484844e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 4.709406766778557e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 1.2839328746849787e-06\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 6.023006335453829e-06\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 7.300157903955551e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 2.5048966563190334e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 1.6395470083807595e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 7.171744300649152e-07\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 9.265975222660927e-07\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 1.3023097835684894e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 1.7726449641486397e-06\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 9.380117944601807e-07\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 8.866192615641921e-07\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 2.167125330743147e-06\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 5.153768825039151e-07\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 5.079008928987605e-07\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 5.810750849377655e-07\n",
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 3.7466460867108253e-07\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 4.796640951099107e-07\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 6.665808882644342e-07\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 2.9193910222602426e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 3.23070537433523e-07\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 3.045025778192212e-07\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 3.118743165941851e-07\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 2.903423990119336e-07\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 5.307425681166933e-07\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 2.428953678190737e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 2.728553454289795e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 3.5214983995501825e-07\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 3.751107442440116e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 3.529084722231346e-07\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 2.428672019050282e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 2.411545665381709e-07\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 2.868295041480451e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 2.848131543942145e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 2.677861914435198e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 2.1538660632813844e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 2.033991535199675e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 3.292640826657589e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.3491514866691432e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.1445413267429103e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.3715868735507684e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 2.4382796937061357e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 2.13937440207701e-07\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.047250546866053e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.0257206756468804e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.0039358616941172e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 2.0484174001467181e-07\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 2.0773282471964194e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.1411615591659938e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 1.995106515551015e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.0523550858797535e-07\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 2.0224452157435735e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 1.8827707037871733e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 2.1850631526376674e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.850814328463457e-07\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 1.9117049987471546e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.8566682058462902e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 1.888413976303127e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 1.870037493745258e-07\n",
      "Training model with 50 nodes in hidden layer\n",
      "Epoch [100/10000], Learning Rate: 0.1, Loss: 0.0015102511970326304\n",
      "Epoch [200/10000], Learning Rate: 0.1, Loss: 0.003760200459510088\n",
      "Epoch [300/10000], Learning Rate: 0.08000000000000002, Loss: 0.003514197887852788\n",
      "Epoch [400/10000], Learning Rate: 0.08000000000000002, Loss: 0.002676001051440835\n",
      "Epoch [500/10000], Learning Rate: 0.06400000000000002, Loss: 0.003679027548059821\n",
      "Epoch [600/10000], Learning Rate: 0.06400000000000002, Loss: 0.011480781249701977\n",
      "Epoch [700/10000], Learning Rate: 0.06400000000000002, Loss: 0.002577299950644374\n",
      "Epoch [800/10000], Learning Rate: 0.051200000000000016, Loss: 0.0017409364227205515\n",
      "Epoch [900/10000], Learning Rate: 0.051200000000000016, Loss: 0.0007606748840771616\n",
      "Epoch [1000/10000], Learning Rate: 0.04096000000000002, Loss: 0.00208398443646729\n",
      "Epoch [1100/10000], Learning Rate: 0.04096000000000002, Loss: 0.0031709703616797924\n",
      "Epoch [1200/10000], Learning Rate: 0.04096000000000002, Loss: 0.0012503837933763862\n",
      "Epoch [1300/10000], Learning Rate: 0.03276800000000001, Loss: 0.0002758835325948894\n",
      "Epoch [1400/10000], Learning Rate: 0.03276800000000001, Loss: 0.0019322502193972468\n",
      "Epoch [1500/10000], Learning Rate: 0.026214400000000013, Loss: 0.0007879696786403656\n",
      "Epoch [1600/10000], Learning Rate: 0.026214400000000013, Loss: 0.00037360351416282356\n",
      "Epoch [1700/10000], Learning Rate: 0.026214400000000013, Loss: 0.00019754318054765463\n",
      "Epoch [1800/10000], Learning Rate: 0.02097152000000001, Loss: 0.0004274684761185199\n",
      "Epoch [1900/10000], Learning Rate: 0.02097152000000001, Loss: 0.00027149487868882716\n",
      "Epoch [2000/10000], Learning Rate: 0.016777216000000008, Loss: 0.0001508909772383049\n",
      "Epoch [2100/10000], Learning Rate: 0.016777216000000008, Loss: 0.00012556719593703747\n",
      "Epoch [2200/10000], Learning Rate: 0.016777216000000008, Loss: 7.44449207559228e-05\n",
      "Epoch [2300/10000], Learning Rate: 0.013421772800000007, Loss: 2.6262348910677247e-05\n",
      "Epoch [2400/10000], Learning Rate: 0.013421772800000007, Loss: 9.021173900691792e-05\n",
      "Epoch [2500/10000], Learning Rate: 0.010737418240000006, Loss: 2.9963732231408358e-05\n",
      "Epoch [2600/10000], Learning Rate: 0.010737418240000006, Loss: 1.8482533050701022e-05\n",
      "Epoch [2700/10000], Learning Rate: 0.010737418240000006, Loss: 9.804943692870438e-05\n",
      "Epoch [2800/10000], Learning Rate: 0.008589934592000005, Loss: 4.4761218305211514e-05\n",
      "Epoch [2900/10000], Learning Rate: 0.008589934592000005, Loss: 4.549000550468918e-06\n",
      "Epoch [3000/10000], Learning Rate: 0.0068719476736000045, Loss: 3.368374382262118e-05\n",
      "Epoch [3100/10000], Learning Rate: 0.0068719476736000045, Loss: 1.3082039004075341e-05\n",
      "Epoch [3200/10000], Learning Rate: 0.0068719476736000045, Loss: 2.823929207806941e-05\n",
      "Epoch [3300/10000], Learning Rate: 0.005497558138880004, Loss: 4.741203156299889e-05\n",
      "Epoch [3400/10000], Learning Rate: 0.005497558138880004, Loss: 1.2117465303163044e-05\n",
      "Epoch [3500/10000], Learning Rate: 0.004398046511104004, Loss: 4.478141818253789e-06\n",
      "Epoch [3600/10000], Learning Rate: 0.004398046511104004, Loss: 9.850962669588625e-06\n",
      "Epoch [3700/10000], Learning Rate: 0.004398046511104004, Loss: 1.5442374206031673e-05\n",
      "Epoch [3800/10000], Learning Rate: 0.0035184372088832034, Loss: 7.6056330726714805e-06\n",
      "Epoch [3900/10000], Learning Rate: 0.0035184372088832034, Loss: 2.6052832708955975e-06\n",
      "Epoch [4000/10000], Learning Rate: 0.002814749767106563, Loss: 4.722663561551599e-06\n",
      "Epoch [4100/10000], Learning Rate: 0.002814749767106563, Loss: 2.4617397684778553e-06\n",
      "Epoch [4200/10000], Learning Rate: 0.002814749767106563, Loss: 6.158437827252783e-06\n",
      "Epoch [4300/10000], Learning Rate: 0.0022517998136852503, Loss: 1.4579445632989518e-06\n",
      "Epoch [4400/10000], Learning Rate: 0.0022517998136852503, Loss: 8.435478775936645e-06\n",
      "Epoch [4500/10000], Learning Rate: 0.0018014398509482003, Loss: 1.937942897711764e-06\n",
      "Epoch [4600/10000], Learning Rate: 0.0018014398509482003, Loss: 1.1350550721545005e-06\n",
      "Epoch [4700/10000], Learning Rate: 0.0018014398509482003, Loss: 2.3177594812295865e-06\n",
      "Epoch [4800/10000], Learning Rate: 0.0014411518807585604, Loss: 2.8909121283504646e-06\n",
      "Epoch [4900/10000], Learning Rate: 0.0014411518807585604, Loss: 2.4749508611421334e-06\n",
      "Epoch [5000/10000], Learning Rate: 0.0011529215046068484, Loss: 1.1665837291729986e-06\n",
      "Epoch [5100/10000], Learning Rate: 0.0011529215046068484, Loss: 1.8440388203089242e-06\n",
      "Epoch [5200/10000], Learning Rate: 0.0011529215046068484, Loss: 1.2679660130743287e-06\n",
      "Epoch [5300/10000], Learning Rate: 0.0009223372036854787, Loss: 1.2473525430323207e-06\n",
      "Epoch [5400/10000], Learning Rate: 0.0009223372036854787, Loss: 8.689231094649585e-07\n",
      "Epoch [5500/10000], Learning Rate: 0.000737869762948383, Loss: 7.638564056833275e-07\n",
      "Epoch [5600/10000], Learning Rate: 0.000737869762948383, Loss: 6.61347542063595e-07\n",
      "Epoch [5700/10000], Learning Rate: 0.000737869762948383, Loss: 7.294045758499124e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5800/10000], Learning Rate: 0.0005902958103587065, Loss: 6.772472715965705e-07\n",
      "Epoch [5900/10000], Learning Rate: 0.0005902958103587065, Loss: 1.23795678064198e-06\n",
      "Epoch [6000/10000], Learning Rate: 0.0004722366482869652, Loss: 7.635484280399396e-07\n",
      "Epoch [6100/10000], Learning Rate: 0.0004722366482869652, Loss: 6.810925583522476e-07\n",
      "Epoch [6200/10000], Learning Rate: 0.0004722366482869652, Loss: 3.7818750797669054e-07\n",
      "Epoch [6300/10000], Learning Rate: 0.0003777893186295722, Loss: 4.898626002614037e-07\n",
      "Epoch [6400/10000], Learning Rate: 0.0003777893186295722, Loss: 4.952424319526472e-07\n",
      "Epoch [6500/10000], Learning Rate: 0.00030223145490365774, Loss: 4.468731162887707e-07\n",
      "Epoch [6600/10000], Learning Rate: 0.00030223145490365774, Loss: 3.814731144302641e-07\n",
      "Epoch [6700/10000], Learning Rate: 0.00030223145490365774, Loss: 3.3391270903848635e-07\n",
      "Epoch [6800/10000], Learning Rate: 0.0002417851639229262, Loss: 4.018876609279687e-07\n",
      "Epoch [6900/10000], Learning Rate: 0.0002417851639229262, Loss: 4.0618743923914735e-07\n",
      "Epoch [7000/10000], Learning Rate: 0.00019342813113834098, Loss: 3.1174332093542034e-07\n",
      "Epoch [7100/10000], Learning Rate: 0.00019342813113834098, Loss: 2.949232964510884e-07\n",
      "Epoch [7200/10000], Learning Rate: 0.00019342813113834098, Loss: 5.34396065177134e-07\n",
      "Epoch [7300/10000], Learning Rate: 0.0001547425049106728, Loss: 3.176414793415461e-07\n",
      "Epoch [7400/10000], Learning Rate: 0.0001547425049106728, Loss: 4.537854465525015e-07\n",
      "Epoch [7500/10000], Learning Rate: 0.00012379400392853823, Loss: 3.291889072443155e-07\n",
      "Epoch [7600/10000], Learning Rate: 0.00012379400392853823, Loss: 3.322543307149317e-07\n",
      "Epoch [7700/10000], Learning Rate: 0.00012379400392853823, Loss: 3.0201186973499716e-07\n",
      "Epoch [7800/10000], Learning Rate: 9.903520314283059e-05, Loss: 2.89202858994031e-07\n",
      "Epoch [7900/10000], Learning Rate: 9.903520314283059e-05, Loss: 2.8703837529064913e-07\n",
      "Epoch [8000/10000], Learning Rate: 7.922816251426448e-05, Loss: 3.3703426538522763e-07\n",
      "Epoch [8100/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.658150037859741e-07\n",
      "Epoch [8200/10000], Learning Rate: 7.922816251426448e-05, Loss: 2.903944960053195e-07\n",
      "Epoch [8300/10000], Learning Rate: 6.338253001141159e-05, Loss: 2.7298909799355897e-07\n",
      "Epoch [8400/10000], Learning Rate: 6.338253001141159e-05, Loss: 2.640876459736319e-07\n",
      "Epoch [8500/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.9521200417548243e-07\n",
      "Epoch [8600/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.6579672862681036e-07\n",
      "Epoch [8700/10000], Learning Rate: 5.070602400912927e-05, Loss: 2.725182639551349e-07\n",
      "Epoch [8800/10000], Learning Rate: 4.056481920730342e-05, Loss: 2.6347618131694617e-07\n",
      "Epoch [8900/10000], Learning Rate: 4.056481920730342e-05, Loss: 2.722842680213944e-07\n",
      "Epoch [9000/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.5005130055433256e-07\n",
      "Epoch [9100/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.5043118512257934e-07\n",
      "Epoch [9200/10000], Learning Rate: 3.2451855365842736e-05, Loss: 2.5192295538545295e-07\n",
      "Epoch [9300/10000], Learning Rate: 2.596148429267419e-05, Loss: 2.638381886299612e-07\n",
      "Epoch [9400/10000], Learning Rate: 2.596148429267419e-05, Loss: 2.5755286969797453e-07\n",
      "Epoch [9500/10000], Learning Rate: 2.0769187434139353e-05, Loss: 2.5953829663194483e-07\n",
      "Epoch [9600/10000], Learning Rate: 2.0769187434139353e-05, Loss: 2.5145291715489293e-07\n",
      "Epoch [9700/10000], Learning Rate: 2.0769187434139353e-05, Loss: 2.4597753167654446e-07\n",
      "Epoch [9800/10000], Learning Rate: 1.6615349947311485e-05, Loss: 2.4925878960857517e-07\n",
      "Epoch [9900/10000], Learning Rate: 1.6615349947311485e-05, Loss: 2.4788985797385976e-07\n",
      "Epoch [10000/10000], Learning Rate: 1.3292279957849188e-05, Loss: 2.475399867307715e-07\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for d in ds:\n",
    "    train(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5c8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fa524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134e0e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
