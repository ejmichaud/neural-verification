{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_verification import (\n",
    "    Tokenizer, \n",
    "    TransformerConfig, \n",
    "    Transformer, \n",
    "    cycle, \n",
    "    HeterogeneousDataLoader\n",
    ")\n",
    "from config import tokenizer_vocabulary, model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt')\n",
    "tokenizer = Tokenizer(tokenizer_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_embed = model.transformer.wte.weight.detach().cpu().numpy()\n",
    "pos_embed = model.transformer.wpe.weight.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_vocabulary\n",
    "token_ids = []\n",
    "for tok in tokenizer_vocabulary: \n",
    "    token_ids.append(tokenizer.encode([tok]))\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_tokens_euclidean(embedding, embeddings, tokenizer, topk=10):\n",
    "    # Calculate distances.\n",
    "    distances = torch.norm(embeddings - embedding, dim=1)\n",
    "    # Get the top k smallest distances.\n",
    "    sims, indices = torch.topk(distances, topk, largest=False)\n",
    "    \n",
    "    # Decode the token IDs to tokens.\n",
    "    closest_tokens = [tokenizer.decode([token_id]) for token_id in indices]\n",
    "    return {\n",
    "        sims[i].item(): closest_tokens[i]\n",
    "        for i in range(len(closest_tokens))\n",
    "    }\n",
    "\n",
    "def get_closest_tokens_cosine(embedding, embeddings, tokenizer, topk=10):\n",
    "    # Calculate similarities. We use cosine similarity here.\n",
    "    similarities = F.cosine_similarity(embedding.unsqueeze(0), embeddings)\n",
    "    # Get the top k indices.\n",
    "    sims, indices = torch.topk(similarities, topk)\n",
    "    \n",
    "    # Decode the token IDs to tokens.\n",
    "    closest_tokens = [tokenizer.decode([token_id]) for token_id in indices]\n",
    "    return {\n",
    "        sims[i].item(): closest_tokens[i]\n",
    "        for i in range(len(closest_tokens))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for token_id in token_ids:\n",
    "    embeddings.append(tok_embed[token_id])\n",
    "    \n",
    "# Convert list to numpy arrays\n",
    "embeddings = np.array(embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
